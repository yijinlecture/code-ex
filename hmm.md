
상태마다 피처가 어떤 분포로 퍼져있는지를 반영해야함
상태별 데이터 분포를 학습하고 
주어진 시계열이 어떤 상태열을 거쳤는지를 가장 가능성 높은 경로로 추론
1. win(윈도우 크기) : 너무 작으면 오탐 가능성 커짐. 너무 크면 둔감해질 수 있음.
    - 5분 단위로 30분을 윈도우로 지정하면 6, 60분을 윈도우로 지정하면 5*12=60이라서 12
    - 짧은 윈도우는 최신 변화에 민감하고, 긴 위도우는 노이즈를 줄이고 안정적으로 추세를 보여줌
    - [추천] 5분 간격이라서 6, 12 부터 시작해서 도메인 지식에 맞게 조정 가능할듯
2. covariance_type(공분산 형태)
    숨겨진 상태마다 관측값이 어떤 분포로 퍼져있는가를 반영함
    분포를 반영하면 각 상태가 평균 수준이 어디쯤인지, 흩어짐은 큰지작은지, 피처 간 상관관계가 있는지 없는지

    - "full": 피처 간 모든 상관관계(공분산)를 허용 -> 유연하지만 파라미터 수가 많아 복잡·느림
        - 데이터 크기가 충분하면 세밀하게 상태를 나누지만, 데이터 적으면 과적합/불안정 가능
    - "diag": 대각행렬(피처 간 독립 가정) ->간단, 학습 안정적, 빠름
        - 계산 빠르고 안정적, 데이터 적을 때 유리, 대신 피처 간 상관관계는 무시됨    
    - "spherical": 모든 피처 분산이 같다고 가정 -> 가장 단순, 잘 안 씀
    - "tied": 모든 상태가 같은 공분산을 공유 -> 파라미터 줄이기 목적

    *데이터 크기와 피처 수에 따라 선택: 피처가 많고 데이터가 적으면 "diag" 권장.
3. EM 반복 횟수(n_iter)
    em알고리즘 : 현재 파라미터로 상태 추정하고 그 추정으로 파라미터를 다시 추정하는걸 번갈아 하며 점점 맞춰가는 것.
    - em알고리즘을 반복하면서 상태/전이확률/분포 파라미터를 추정하기 때문에 이걸 얼마나 반복해서 돌릴지 정하는 값
    - 반복 적으면 최적해 도달 전에 학습 종료
    - 반복 충분하면 상태 분리/전이확률이 안정화됨

    - n_iter 작게 : 실행은 빠르지만, 상태 분리가 애매하거나 수렴 경고 뜰 수 있음
    - n_iter 크게 : 안정적 수렴 가능성 up, 하지만 계산시간 증가    
